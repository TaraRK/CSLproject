{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaraRK/CSLproject/blob/main/decision-mamba-hf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qntpWiEWX89"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf \\\n",
        "    xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuXIPJUWVrhq"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.21.0\n",
        "!pip install free-mujoco-py\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install imageio-ffmpeg\n",
        "\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install xvfbwrapper\n",
        "!pip install imageio==2.4.1\n",
        "!pip install imageio-ffmpeg\n",
        "!pip install huggingface_hub\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "rY8Xx6519dpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DktITQNXTopc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3bLeIHqUwq7"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
        "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1QzZHmPUM4p"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DecisionTransformerGymDataCollator:\n",
        "    return_tensors: str = \"pt\"\n",
        "    max_len: int = 20 #subsets of the episode we use for training\n",
        "    state_dim: int = 17  # size of state space\n",
        "    act_dim: int = 6  # size of action space\n",
        "    max_ep_len: int = 1000 # max episode length in the dataset\n",
        "    scale: float = 1000.0  # normalization of rewards/returns\n",
        "    state_mean: np.array = None  # to store state means\n",
        "    state_std: np.array = None  # to store state stds\n",
        "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
        "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
        "\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
        "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
        "        self.dataset = dataset\n",
        "        # calculate dataset stats for normalization of states\n",
        "        states = []\n",
        "        traj_lens = []\n",
        "        for obs in dataset[\"observations\"]:\n",
        "            states.extend(obs)\n",
        "            traj_lens.append(len(obs))\n",
        "        self.n_traj = len(traj_lens)\n",
        "        states = np.vstack(states)\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "        traj_lens = np.array(traj_lens)\n",
        "        self.p_sample = traj_lens / sum(traj_lens)\n",
        "\n",
        "    def _discount_cumsum(self, x, gamma):\n",
        "        discount_cumsum = np.zeros_like(x)\n",
        "        discount_cumsum[-1] = x[-1]\n",
        "        for t in reversed(range(x.shape[0] - 1)):\n",
        "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
        "        return discount_cumsum\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch_size = len(features)\n",
        "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
        "        batch_inds = np.random.choice(\n",
        "            np.arange(self.n_traj),\n",
        "            size=batch_size,\n",
        "            replace=True,\n",
        "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
        "        )\n",
        "        # a batch of dataset features\n",
        "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
        "\n",
        "        for ind in batch_inds:\n",
        "            # for feature in features:\n",
        "            feature = self.dataset[int(ind)]\n",
        "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
        "\n",
        "            # get sequences from dataset\n",
        "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
        "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
        "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
        "\n",
        "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
        "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
        "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
        "            rtg.append(\n",
        "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
        "                    : s[-1].shape[1]   # TODO check the +1 removed here\n",
        "                ].reshape(1, -1, 1)\n",
        "            )\n",
        "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
        "                print(\"if true\")\n",
        "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
        "\n",
        "            # padding and state + reward normalization\n",
        "            tlen = s[-1].shape[1]\n",
        "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
        "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
        "            a[-1] = np.concatenate(\n",
        "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
        "                axis=1,\n",
        "            )\n",
        "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
        "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
        "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
        "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
        "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
        "\n",
        "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
        "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
        "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
        "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
        "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
        "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
        "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
        "\n",
        "        return {\n",
        "            \"states\": s,\n",
        "            \"actions\": a,\n",
        "            \"rewards\": r,\n",
        "            \"returns_to_go\": rtg,\n",
        "            \"timesteps\": timesteps,\n",
        "            \"attention_mask\": mask,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers.utils import (\n",
        "    ModelOutput,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from transformers import PretrainedConfig, DecisionTransformerPreTrainedModel\n",
        "\n",
        "@dataclass\n",
        "class DecisionTransformerOutput(ModelOutput):\n",
        "    state_preds: torch.FloatTensor = None\n",
        "    action_preds: torch.FloatTensor = None\n",
        "    return_preds: torch.FloatTensor = None\n",
        "    hidden_states: torch.FloatTensor = None\n",
        "    attentions: torch.FloatTensor = None\n",
        "    last_hidden_state: torch.FloatTensor = None"
      ],
      "metadata": {
        "id": "LgqoKDX28A6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionMambaConfig(PretrainedConfig):\n",
        "    \"\"\"\n",
        "    This is the configuration class to store the configuration of a [`DecisionTransformerModel`]. It is used to\n",
        "    instantiate a Decision Transformer model according to the specified arguments, defining the model architecture.\n",
        "    Instantiating a configuration with the defaults will yield a similar configuration to that of the standard\n",
        "    DecisionTransformer architecture. Many of the config options are used to instatiate the GPT2 model that is used as\n",
        "    part of the architecture.\n",
        "\n",
        "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
        "    documentation from [`PretrainedConfig`] for more information.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        state_dim (`int`, *optional*, defaults to 17):\n",
        "            The state size for the RL environment\n",
        "        act_dim (`int`, *optional*, defaults to 4):\n",
        "            The size of the output action space\n",
        "        hidden_size (`int`, *optional*, defaults to 128):\n",
        "            The size of the hidden layers\n",
        "        max_ep_len (`int`, *optional*, defaults to 4096):\n",
        "            The maximum length of an episode in the environment\n",
        "        action_tanh (`bool`, *optional*, defaults to True):\n",
        "            Whether to use a tanh activation on action prediction\n",
        "        vocab_size (`int`, *optional*, defaults to 50257):\n",
        "            Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the\n",
        "            `inputs_ids` passed when calling [`DecisionTransformerModel`].\n",
        "        n_positions (`int`, *optional*, defaults to 1024):\n",
        "            The maximum sequence length that this model might ever be used with. Typically set this to something large\n",
        "            just in case (e.g., 512 or 1024 or 2048).\n",
        "        n_layer (`int`, *optional*, defaults to 3):\n",
        "            Number of hidden layers in the Transformer encoder.\n",
        "        n_head (`int`, *optional*, defaults to 1):\n",
        "            Number of attention heads for each attention layer in the Transformer encoder.\n",
        "        n_inner (`int`, *optional*):\n",
        "            Dimensionality of the inner feed-forward layers. If unset, will default to 4 times `n_embd`.\n",
        "        activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n",
        "            Activation function, to be selected in the list `[\"relu\", \"silu\", \"gelu\", \"tanh\", \"gelu_new\"]`.\n",
        "        resid_pdrop (`float`, *optional*, defaults to 0.1):\n",
        "            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n",
        "        embd_pdrop (`int`, *optional*, defaults to 0.1):\n",
        "            The dropout ratio for the embeddings.\n",
        "        attn_pdrop (`float`, *optional*, defaults to 0.1):\n",
        "            The dropout ratio for the attention.\n",
        "        layer_norm_epsilon (`float`, *optional*, defaults to 1e-5):\n",
        "            The epsilon to use in the layer normalization layers.\n",
        "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
        "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "        scale_attn_weights (`bool`, *optional*, defaults to `True`):\n",
        "            Scale attention weights by dividing by sqrt(hidden_size)..\n",
        "        use_cache (`bool`, *optional*, defaults to `True`):\n",
        "            Whether or not the model should return the last key/values attentions (not used by all models).\n",
        "        scale_attn_by_inverse_layer_idx (`bool`, *optional*, defaults to `False`):\n",
        "            Whether to additionally scale attention weights by `1 / layer_idx + 1`.\n",
        "        reorder_and_upcast_attn (`bool`, *optional*, defaults to `False`):\n",
        "            Whether to scale keys (K) prior to computing attention (dot-product) and upcast attention\n",
        "            dot-product/softmax to float() when training with mixed precision.\n",
        "\n",
        "    Example:\n",
        "\n",
        "    ```python\n",
        "    >>> from transformers import DecisionTransformerConfig, DecisionTransformerModel\n",
        "\n",
        "    >>> # Initializing a DecisionTransformer configuration\n",
        "    >>> configuration = DecisionTransformerConfig()\n",
        "\n",
        "    >>> # Initializing a model (with random weights) from the configuration\n",
        "    >>> model = DecisionTransformerModel(configuration)\n",
        "\n",
        "    >>> # Accessing the model configuration\n",
        "    >>> configuration = model.config\n",
        "    ```\"\"\"\n",
        "\n",
        "    model_type = \"decision_transformer\"\n",
        "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
        "    attribute_map = {\n",
        "        \"max_position_embeddings\": \"n_positions\",\n",
        "        \"num_attention_heads\": \"n_head\",\n",
        "        \"num_hidden_layers\": \"n_layer\",\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim=17,\n",
        "        act_dim=4,\n",
        "        max_ep_len=4096,\n",
        "        action_tanh=True,\n",
        "        n_positions=1024,\n",
        "        vocab_size=1,\n",
        "        hidden_size=128, # 768 orig\n",
        "        state_size=16,\n",
        "        num_hidden_layers=3, # 32 orig\n",
        "        layer_norm_epsilon=1e-5,\n",
        "        pad_token_id=50256,\n",
        "        bos_token_id=50256,\n",
        "        eos_token_id=50256,\n",
        "        expand=2,\n",
        "        conv_kernel=4,\n",
        "        use_bias=False,\n",
        "        use_conv_bias=True,\n",
        "        hidden_act=\"silu\", # DT used to be relu\n",
        "        initializer_range=0.1, # DT had 0.02\n",
        "        residual_in_fp32=True,\n",
        "        time_step_rank=\"auto\",\n",
        "        time_step_scale=1.0,\n",
        "        time_step_min=0.001,\n",
        "        time_step_max=0.1,\n",
        "        time_step_init_scheme=\"random\",\n",
        "        time_step_floor=1e-4,\n",
        "        rescale_prenorm_residual=False,\n",
        "        use_cache=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # dt\n",
        "        self.state_dim = state_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.max_ep_len = max_ep_len\n",
        "        self.action_tanh = action_tanh\n",
        "        self.n_positions = n_positions\n",
        "\n",
        "        # mamba\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.state_size = state_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\n",
        "        self.conv_kernel = conv_kernel\n",
        "        self.expand = expand\n",
        "        self.intermediate_size = int(expand * self.hidden_size)\n",
        "        self.bos_token_id = bos_token_id\n",
        "        self.eos_token_id = eos_token_id\n",
        "        self.pad_token_id = pad_token_id\n",
        "        self.use_bias = use_bias\n",
        "        self.use_conv_bias = use_conv_bias\n",
        "        self.hidden_act = hidden_act\n",
        "        self.initializer_range = initializer_range\n",
        "        self.time_step_rank = math.ceil(self.hidden_size / 16) if time_step_rank == \"auto\" else time_step_rank\n",
        "        self.time_step_scale = time_step_scale\n",
        "        self.time_step_min = time_step_min\n",
        "        self.time_step_max = time_step_max\n",
        "        self.time_step_init_scheme = time_step_init_scheme\n",
        "        self.time_step_floor = time_step_floor\n",
        "        self.rescale_prenorm_residual = rescale_prenorm_residual\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "        self.use_cache = use_cache\n",
        "\n",
        "        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n",
        "\n",
        "\"\"\"\n",
        "This is the configuration class to store the configuration of a [`MambaModel`]. It is used to instantiate a MAMBA\n",
        "model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
        "defaults will yield a similar configuration to that of the MAMBA\n",
        "[state-spaces/mamba-2.8b](https://huggingface.co/state-spaces/mamba-2.8b) architecture.\n",
        "\n",
        "Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
        "documentation from [`PretrainedConfig`] for more information.\n",
        "\n",
        "\n",
        "Args:\n",
        "    vocab_size (`int`, *optional*, defaults to 50280):\n",
        "        Vocabulary size of the MAMBA model. Defines the number of different tokens that can be represented by the\n",
        "        `inputs_ids` passed when calling [`MambaModel`].\n",
        "    hidden_size (`int`, *optional*, defaults to 768):\n",
        "        Dimensionality of the embeddings and hidden states.\n",
        "    state_size (`int`, *optional*, defaults to 16): shape of the state space latents.\n",
        "    num_hidden_layers (`int`, *optional*, defaults to 32):\n",
        "        Number of hidden layers in the model.\n",
        "    layer_norm_epsilon (`float`, *optional*, defaults to 1e-05):\n",
        "        The epsilon to use in the layer normalization layers.\n",
        "    pad_token_id (`int`, *optional*, defaults to 0):\n",
        "        Padding token id.\n",
        "    bos_token_id (`int`, *optional*, defaults to 0):\n",
        "        The id of the beginning of sentence token in the vocabulary.\n",
        "    eos_token_id (`int`, *optional*, defaults to 0):\n",
        "        The id of the end of sentence token in the vocabulary.\n",
        "    expand (`int`, *optional*, defaults to 2): Expanding factor used to determine the intermediate size.\n",
        "    conv_kernel (`int`, *optional*, defaults to 4): Size of the convolution kernel.\n",
        "    use_bias (`bool`, *optional*, defaults to `False`):\n",
        "        Whether or not to use bias in [\"in_proj\", \"out_proj\"] of the mixer block\n",
        "    use_conv_bias (`bool`, *optional*, defaults to `True`):\n",
        "        Whether or not to use bias in the convolution layer of the mixer block.\n",
        "    hidden_act (`str`, *optional*, defaults to `\"silu\"`):\n",
        "        The non-linear activation function (function or string) in the decoder.\n",
        "    initializer_range (`float`, *optional*, defaults to 0.1):\n",
        "        The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
        "    residual_in_fp32 (`bool`, *optional*, defaults to `True`):\n",
        "        Whether or not residuals should be in `float32`. If set to `False` residuals will keep the same `dtype` as the rest of the model\n",
        "    time_step_rank (`Union[int,str]`, *optional*, defaults to `\"auto\"`):\n",
        "        Rank of the discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n",
        "    time_step_scale (`float`, *optional*, defaults to 1.0):\n",
        "        Scale used used to scale `dt_proj.bias`.\n",
        "    time_step_min (`float`, *optional*, defaults to 0.001):\n",
        "        Minimum `time_step` used to bound `dt_proj.bias`.\n",
        "    time_step_max (`float`, *optional*, defaults to 0.1):\n",
        "        Maximum `time_step` used to bound `dt_proj.bias`.\n",
        "    time_step_init_scheme (`float`, *optional*, defaults to `\"random\"`):\n",
        "        Init scheme used for `dt_proj.weight`. Should be one of `[\"random\",\"uniform\"]`\n",
        "    time_step_floor (`float`, *optional*, defaults to 0.0001):\n",
        "        Minimum clamping value of the `dt_proj.bias` layer initialization.\n",
        "    rescale_prenorm_residual (`bool`, *optional*, defaults to `False`):\n",
        "        Whether or not to rescale `out_proj` weights when initializing.\n",
        "    use_cache (`bool`, *optional*, defaults to `True`):\n",
        "        Whether or not the cache should be used.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        ">>> from transformers import MambaConfig, MambaModel\n",
        "\n",
        ">>> # Initializing a Mamba configuration\n",
        ">>> configuration = MambaConfig()\n",
        "\n",
        ">>> # Initializing a model (with random weights) from the configuration\n",
        ">>> model = MambaModel(configuration)\n",
        "\n",
        ">>> # Accessing the model configuration\n",
        ">>> configuration = model.config\n",
        "```\"\"\""
      ],
      "metadata": {
        "id": "u4sZj4wy4N0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Model, MambaModel\n",
        "\n",
        "class DecisionTransformerModel(DecisionTransformerPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model builds upon the GPT2 architecture to perform autoregressive prediction of actions in an offline RL\n",
        "    setting. Refer to the paper for more details: https://arxiv.org/abs/2106.01345\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "        self.hidden_size = config.hidden_size\n",
        "        # note: the only difference between this GPT2Model and the default Huggingface version\n",
        "        # is that the positional embeddings are removed (since we'll add those ourselves)\n",
        "        self.encoder = MambaModel(config)\n",
        "\n",
        "        self.embed_timestep = nn.Embedding(config.max_ep_len, config.hidden_size)\n",
        "        self.embed_return = torch.nn.Linear(1, config.hidden_size)\n",
        "        self.embed_state = torch.nn.Linear(config.state_dim, config.hidden_size)\n",
        "        self.embed_action = torch.nn.Linear(config.act_dim, config.hidden_size)\n",
        "\n",
        "        self.embed_ln = nn.LayerNorm(config.hidden_size)\n",
        "\n",
        "        # note: we don't predict states or returns for the paper\n",
        "        self.predict_state = torch.nn.Linear(config.hidden_size, config.state_dim)\n",
        "        self.predict_action = nn.Sequential(\n",
        "            *([nn.Linear(config.hidden_size, config.act_dim)] + ([nn.Tanh()] if config.action_tanh else []))\n",
        "        )\n",
        "        self.predict_return = torch.nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        states: Optional[torch.FloatTensor] = None,\n",
        "        actions: Optional[torch.FloatTensor] = None,\n",
        "        rewards: Optional[torch.FloatTensor] = None,\n",
        "        returns_to_go: Optional[torch.FloatTensor] = None,\n",
        "        timesteps: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.FloatTensor], DecisionTransformerOutput]:\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        batch_size, seq_length = states.shape[0], states.shape[1]\n",
        "\n",
        "        if attention_mask is None:\n",
        "            # attention mask for GPT: 1 if can be attended to, 0 if not\n",
        "            attention_mask = torch.ones((batch_size, seq_length), dtype=torch.long)\n",
        "\n",
        "        # embed each modality with a different head\n",
        "        state_embeddings = self.embed_state(states)\n",
        "        action_embeddings = self.embed_action(actions)\n",
        "        returns_embeddings = self.embed_return(returns_to_go)\n",
        "        time_embeddings = self.embed_timestep(timesteps)\n",
        "\n",
        "        # time embeddings are treated similar to positional embeddings\n",
        "        state_embeddings = state_embeddings + time_embeddings\n",
        "        action_embeddings = action_embeddings + time_embeddings\n",
        "        returns_embeddings = returns_embeddings + time_embeddings\n",
        "\n",
        "        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n",
        "        # which works nice in an autoregressive sense since states predict actions\n",
        "        stacked_inputs = (\n",
        "            torch.stack((returns_embeddings, state_embeddings, action_embeddings), dim=1)\n",
        "            .permute(0, 2, 1, 3)\n",
        "            .reshape(batch_size, 3 * seq_length, self.hidden_size)\n",
        "        )\n",
        "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
        "\n",
        "        # to make the attention mask fit the stacked inputs, have to stack it as well\n",
        "        stacked_attention_mask = (\n",
        "            torch.stack((attention_mask, attention_mask, attention_mask), dim=1)\n",
        "            .permute(0, 2, 1)\n",
        "            .reshape(batch_size, 3 * seq_length)\n",
        "        )\n",
        "        device = stacked_inputs.device\n",
        "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
        "        encoder_outputs = self.encoder(\n",
        "            inputs_embeds=stacked_inputs,\n",
        "            attention_mask=stacked_attention_mask,\n",
        "            position_ids=torch.zeros(stacked_attention_mask.shape, device=device, dtype=torch.long),\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        x = encoder_outputs[0]\n",
        "\n",
        "        # reshape x so that the second dimension corresponds to the original\n",
        "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
        "        x = x.reshape(batch_size, seq_length, 3, self.hidden_size).permute(0, 2, 1, 3)\n",
        "\n",
        "        # get predictions\n",
        "        return_preds = self.predict_return(x[:, 2])  # predict next return given state and action\n",
        "        state_preds = self.predict_state(x[:, 2])  # predict next state given state and action\n",
        "        action_preds = self.predict_action(x[:, 1])  # predict next action given state\n",
        "        if not return_dict:\n",
        "            return (state_preds, action_preds, return_preds)\n",
        "\n",
        "        return DecisionTransformerOutput(\n",
        "            last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            state_preds=state_preds,\n",
        "            action_preds=action_preds,\n",
        "            return_preds=return_preds,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "        )"
      ],
      "metadata": {
        "id": "nRZ6KAsR8M2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZp7hhFUh5u"
      },
      "outputs": [],
      "source": [
        "class TrainableDT(DecisionTransformerModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        output = super().forward(**kwargs)\n",
        "        # add the DT loss\n",
        "        action_preds = output[1]\n",
        "        action_targets = kwargs[\"actions\"]\n",
        "        attention_mask = kwargs[\"attention_mask\"]\n",
        "        act_dim = action_preds.shape[2]\n",
        "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "\n",
        "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def original_forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIJCY3b3pQAh"
      },
      "outputs": [],
      "source": [
        "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
        "\n",
        "config = DecisionMambaConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
        "model = TrainableDT(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "OGgUz0y631vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: prit trainable parameters in model\n",
        "\n",
        "trainable_params = 0\n",
        "total_params = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params += param.numel()\n",
        "    total_params += param.numel()\n",
        "print(f\"Trainable parameters: {trainable_params}\")\n",
        "print(f\"Total parameters: {total_params}\")\n",
        "\n",
        "model"
      ],
      "metadata": {
        "id": "jPfJsGy43Mah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNzzKWuuU9I4"
      },
      "outputs": [],
      "source": [
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"output/\",\n",
        "#     remove_unused_columns=False,\n",
        "#     num_train_epochs=120,\n",
        "#     per_device_train_batch_size=64,\n",
        "#     learning_rate=1e-4,\n",
        "#     weight_decay=1e-4,\n",
        "#     warmup_ratio=0.1,\n",
        "#     optim=\"adamw_torch\",\n",
        "#     max_grad_norm=0.25,\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=dataset[\"train\"],\n",
        "#     data_collator=collator,\n",
        "# )\n",
        "\n",
        "# trainer.train()\n",
        "\n",
        "model = TrainableDT.from_pretrained('temp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NhIn4up26c"
      },
      "outputs": [],
      "source": [
        "import mujoco_py\n",
        "import gym\n",
        "\n",
        "from colabgymrender.recorder import Recorder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-izl68BqUG5"
      },
      "outputs": [],
      "source": [
        "# Function that gets an action from the model using autoregressive prediction with a window of the previous 20 timesteps.\n",
        "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
        "    # This implementation does not condition on past rewards\n",
        "\n",
        "    states = states.reshape(1, -1, model.config.state_dim)\n",
        "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
        "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
        "    timesteps = timesteps.reshape(1, -1)\n",
        "\n",
        "    states = states[:, -model.config.max_length :]\n",
        "    actions = actions[:, -model.config.max_length :]\n",
        "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
        "    timesteps = timesteps[:, -model.config.max_length :]\n",
        "    padding = model.config.max_length - states.shape[1]\n",
        "    # pad all tokens to sequence length\n",
        "    attention_mask = torch.cat([torch.zeros(padding), torch.ones(states.shape[1])])\n",
        "    attention_mask = attention_mask.to(dtype=torch.long).reshape(1, -1)\n",
        "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim)), states], dim=1).float()\n",
        "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim)), actions], dim=1).float()\n",
        "    returns_to_go = torch.cat([torch.zeros((1, padding, 1)), returns_to_go], dim=1).float()\n",
        "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long), timesteps], dim=1)\n",
        "\n",
        "    state_preds, action_preds, return_preds = model.original_forward(\n",
        "        states=states,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        returns_to_go=returns_to_go,\n",
        "        timesteps=timesteps,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict=False,\n",
        "    )\n",
        "\n",
        "    return action_preds[0, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPuiNy-qWnP"
      },
      "outputs": [],
      "source": [
        "# build the environment\n",
        "directory = './video'\n",
        "model = model.to(\"cpu\")\n",
        "env = gym.make(\"HalfCheetah-v3\")\n",
        "env = Recorder(env, directory, fps=30)\n",
        "max_ep_len = 500\n",
        "device = \"cpu\"\n",
        "scale = 500.0  # normalization for rewards/returns\n",
        "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
        "\n",
        "state_mean = collator.state_mean.astype(np.float32)\n",
        "state_std = collator.state_std.astype(np.float32)\n",
        "print(state_mean)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "# Create the decision transformer model\n",
        "\n",
        "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
        "state_std = torch.from_numpy(state_std).to(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysPH9rtRqY-g"
      },
      "outputs": [],
      "source": [
        "# Interact with the environment and create a video\n",
        "episode_return, episode_length = 0, 0\n",
        "state = env.reset()\n",
        "target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
        "states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
        "actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
        "rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
        "\n",
        "timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
        "for t in tqdm(range(max_ep_len)):\n",
        "    actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
        "    rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
        "\n",
        "    action = get_action(\n",
        "        model,\n",
        "        (states - state_mean) / state_std,\n",
        "        actions,\n",
        "        rewards,\n",
        "        target_return,\n",
        "        timesteps,\n",
        "    )\n",
        "    actions[-1] = action\n",
        "    action = action.detach().cpu().numpy()\n",
        "\n",
        "    state, reward, done, _ = env.step(action)\n",
        "\n",
        "    cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
        "    states = torch.cat([states, cur_state], dim=0)\n",
        "    rewards[-1] = reward\n",
        "\n",
        "    pred_return = target_return[0, -1] - (reward / scale)\n",
        "    target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
        "    timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
        "\n",
        "    episode_return += reward\n",
        "    episode_length += 1\n",
        "\n",
        "    if done:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rewards.mean()"
      ],
      "metadata": {
        "id": "fPgdJInlZn_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1Zy7x9qbnI"
      },
      "outputs": [],
      "source": [
        "# Play the video\n",
        "env = Recorder(env)\n",
        "env.play()\n",
        "# If you want to convert the video:\n",
        "# !ffmpeg -i {your_video} -vcodec h264 replay.mp4"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}